#!/usr/bin/env python3
"""
å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–å™¨
Author: LDL
Date: 2025-01-25

ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è‡ªåŠ¨ä¼˜åŒ–äº¤æ˜“ç­–ç•¥å‚æ•°å’Œå†³ç­–
"""

import numpy as np
import pandas as pd
import gymnasium as gym
from gymnasium import spaces
import random
from collections import deque
import warnings
warnings.filterwarnings('ignore')

class TradingEnvironment(gym.Env):
    """äº¤æ˜“ç¯å¢ƒ - å¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒç¯å¢ƒ"""
    
    def __init__(self, data, initial_balance=100000, transaction_cost=0.001):
        super(TradingEnvironment, self).__init__()
        
        self.data = data.reset_index(drop=True)
        self.initial_balance = initial_balance
        self.transaction_cost = transaction_cost
        
        # åŠ¨ä½œç©ºé—´ï¼š0=æŒæœ‰, 1=ä¹°å…¥, 2=å–å‡º
        self.action_space = spaces.Discrete(3)
        
        # çŠ¶æ€ç©ºé—´ï¼šä»·æ ¼ç‰¹å¾ + æŒä»“ä¿¡æ¯
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=(10,), dtype=np.float32
        )
        
        self.reset()
    
    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        self.current_step = 20  # ä»ç¬¬20å¤©å¼€å§‹ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®
        self.balance = self.initial_balance
        self.shares_held = 0
        self.net_worth = self.initial_balance
        self.max_net_worth = self.initial_balance
        self.trades = []
        
        return self._get_observation()
    
    def _get_observation(self):
        """è·å–å½“å‰çŠ¶æ€è§‚å¯Ÿ"""
        if self.current_step >= len(self.data):
            return np.zeros(10)
        
        # ä»·æ ¼ç‰¹å¾
        current_price = self.data.loc[self.current_step, 'Close']
        
        # æŠ€æœ¯æŒ‡æ ‡
        prices = self.data.loc[max(0, self.current_step-20):self.current_step, 'Close']
        
        if len(prices) < 2:
            return np.zeros(10)
        
        # è®¡ç®—ç‰¹å¾
        returns = prices.pct_change().fillna(0)
        
        features = [
            current_price / prices.iloc[0] - 1,  # ç›¸å¯¹ä»·æ ¼å˜åŒ–
            returns.iloc[-1],  # æœ€æ–°æ”¶ç›Šç‡
            returns.mean(),  # å¹³å‡æ”¶ç›Šç‡
            returns.std(),  # æ”¶ç›Šç‡æ ‡å‡†å·®
            (current_price - prices.mean()) / prices.std(),  # æ ‡å‡†åŒ–ä»·æ ¼ä½ç½®
            len(prices[prices > current_price]) / len(prices),  # ä»·æ ¼åˆ†ä½æ•°
            self.shares_held / 1000,  # æ ‡å‡†åŒ–æŒä»“
            self.balance / self.initial_balance,  # æ ‡å‡†åŒ–ä½™é¢
            self.net_worth / self.initial_balance,  # æ ‡å‡†åŒ–å‡€å€¼
            (self.net_worth - self.max_net_worth) / self.initial_balance  # å›æ’¤
        ]
        
        return np.array(features, dtype=np.float32)
    
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        if self.current_step >= len(self.data) - 1:
            return self._get_observation(), 0, True, {}
        
        current_price = self.data.loc[self.current_step, 'Close']
        
        # æ‰§è¡ŒåŠ¨ä½œ
        reward = 0
        
        if action == 1:  # ä¹°å…¥
            if self.balance > current_price * (1 + self.transaction_cost):
                shares_to_buy = int(self.balance / (current_price * (1 + self.transaction_cost)))
                cost = shares_to_buy * current_price * (1 + self.transaction_cost)
                self.balance -= cost
                self.shares_held += shares_to_buy
                self.trades.append(('BUY', self.current_step, current_price, shares_to_buy))
        
        elif action == 2:  # å–å‡º
            if self.shares_held > 0:
                revenue = self.shares_held * current_price * (1 - self.transaction_cost)
                self.balance += revenue
                self.trades.append(('SELL', self.current_step, current_price, self.shares_held))
                self.shares_held = 0
        
        # æ›´æ–°å‡€å€¼
        self.net_worth = self.balance + self.shares_held * current_price
        self.max_net_worth = max(self.max_net_worth, self.net_worth)
        
        # ç§»åŠ¨åˆ°ä¸‹ä¸€æ­¥
        self.current_step += 1
        
        # è®¡ç®—å¥–åŠ±
        if self.current_step < len(self.data):
            next_price = self.data.loc[self.current_step, 'Close']
            price_change = (next_price - current_price) / current_price
            
            # åŸºäºæŒä»“å’Œä»·æ ¼å˜åŒ–çš„å¥–åŠ±
            if self.shares_held > 0:
                reward = price_change * 100  # æŒæœ‰æ—¶ï¼Œä»·æ ¼ä¸Šæ¶¨è·å¾—æ­£å¥–åŠ±
            else:
                reward = -price_change * 50  # ç©ºä»“æ—¶ï¼Œä»·æ ¼ä¸‹è·Œè·å¾—æ­£å¥–åŠ±
            
            # æ·»åŠ é£é™©è°ƒæ•´
            drawdown = (self.max_net_worth - self.net_worth) / self.max_net_worth
            reward -= drawdown * 50  # å›æ’¤æƒ©ç½š
        
        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        done = self.current_step >= len(self.data) - 1
        
        return self._get_observation(), reward, done, {
            'net_worth': self.net_worth,
            'balance': self.balance,
            'shares_held': self.shares_held
        }

class DQNAgent:
    """æ·±åº¦Qç½‘ç»œæ™ºèƒ½ä½“"""
    
    def __init__(self, state_size, action_size, learning_rate=0.001):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        self.epsilon = 1.0  # æ¢ç´¢ç‡
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = learning_rate
        
        # ç®€åŒ–çš„Qç½‘ç»œï¼ˆä½¿ç”¨numpyå®ç°ï¼‰
        self.q_table = {}
        
    def remember(self, state, action, reward, next_state, done):
        """è®°ä½ç»éªŒ"""
        self.memory.append((state, action, reward, next_state, done))
    
    def act(self, state):
        """é€‰æ‹©åŠ¨ä½œ"""
        if np.random.random() <= self.epsilon:
            return random.randrange(self.action_size)
        
        # ç®€åŒ–çš„Qå€¼è®¡ç®—
        state_key = tuple(np.round(state, 2))
        if state_key not in self.q_table:
            self.q_table[state_key] = np.random.random(self.action_size)
        
        return np.argmax(self.q_table[state_key])
    
    def replay(self, batch_size=32):
        """ç»éªŒå›æ”¾å­¦ä¹ """
        if len(self.memory) < batch_size:
            return
        
        batch = random.sample(self.memory, batch_size)
        
        for state, action, reward, next_state, done in batch:
            target = reward
            if not done:
                next_state_key = tuple(np.round(next_state, 2))
                if next_state_key not in self.q_table:
                    self.q_table[next_state_key] = np.random.random(self.action_size)
                target = reward + 0.95 * np.amax(self.q_table[next_state_key])
            
            state_key = tuple(np.round(state, 2))
            if state_key not in self.q_table:
                self.q_table[state_key] = np.random.random(self.action_size)
            
            self.q_table[state_key][action] = target
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

class ReinforcementLearningOptimizer:
    """å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.agent = None
        self.env = None
        self.training_history = []
        
        print("ğŸ¤– å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
        print("Author: LDL")
    
    def create_training_environment(self, data, initial_balance=100000):
        """åˆ›å»ºè®­ç»ƒç¯å¢ƒ"""
        print("ğŸ—ï¸ åˆ›å»ºå¼ºåŒ–å­¦ä¹ è®­ç»ƒç¯å¢ƒ...")
        
        self.env = TradingEnvironment(data, initial_balance)
        self.agent = DQNAgent(
            state_size=self.env.observation_space.shape[0],
            action_size=self.env.action_space.n
        )
        
        print(f"âœ… ç¯å¢ƒåˆ›å»ºå®Œæˆ")
        print(f"  çŠ¶æ€ç©ºé—´ç»´åº¦: {self.env.observation_space.shape[0]}")
        print(f"  åŠ¨ä½œç©ºé—´å¤§å°: {self.env.action_space.n}")
        print(f"  åˆå§‹èµ„é‡‘: ${initial_balance:,}")
    
    def train_agent(self, episodes=100):
        """è®­ç»ƒå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“"""
        print(f"ğŸ“ å¼€å§‹è®­ç»ƒå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ ({episodes}è½®)...")
        
        if self.env is None or self.agent is None:
            print("âŒ è¯·å…ˆåˆ›å»ºè®­ç»ƒç¯å¢ƒ")
            return
        
        scores = []
        
        for episode in range(episodes):
            state = self.env.reset()
            total_reward = 0
            steps = 0
            
            while True:
                action = self.agent.act(state)
                next_state, reward, done, info = self.env.step(action)
                
                self.agent.remember(state, action, reward, next_state, done)
                state = next_state
                total_reward += reward
                steps += 1
                
                if done:
                    break
            
            # ç»éªŒå›æ”¾å­¦ä¹ 
            if len(self.agent.memory) > 32:
                self.agent.replay(32)
            
            scores.append(total_reward)
            final_net_worth = info.get('net_worth', 0)
            
            if episode % 20 == 0:
                avg_score = np.mean(scores[-20:]) if len(scores) >= 20 else np.mean(scores)
                print(f"  è½®æ¬¡ {episode}: å¹³å‡å¥–åŠ±={avg_score:.2f}, æœ€ç»ˆå‡€å€¼=${final_net_worth:,.2f}, æ¢ç´¢ç‡={self.agent.epsilon:.3f}")
        
        self.training_history = scores
        print(f"âœ… è®­ç»ƒå®Œæˆï¼å¹³å‡å¥–åŠ±: {np.mean(scores):.2f}")
        
        return scores
    
    def test_strategy(self, test_data):
        """æµ‹è¯•è®­ç»ƒå¥½çš„ç­–ç•¥"""
        print("ğŸ§ª æµ‹è¯•å¼ºåŒ–å­¦ä¹ ç­–ç•¥...")
        
        if self.agent is None:
            print("âŒ è¯·å…ˆè®­ç»ƒæ™ºèƒ½ä½“")
            return None
        
        # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
        test_env = TradingEnvironment(test_data)
        state = test_env.reset()
        
        actions_taken = []
        portfolio_values = []
        
        # è®¾ç½®ä¸ºæµ‹è¯•æ¨¡å¼ï¼ˆä¸æ¢ç´¢ï¼‰
        original_epsilon = self.agent.epsilon
        self.agent.epsilon = 0
        
        while True:
            action = self.agent.act(state)
            next_state, reward, done, info = test_env.step(action)
            
            actions_taken.append(action)
            portfolio_values.append(info['net_worth'])
            
            state = next_state
            
            if done:
                break
        
        # æ¢å¤æ¢ç´¢ç‡
        self.agent.epsilon = original_epsilon
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        initial_value = test_env.initial_balance
        final_value = portfolio_values[-1]
        total_return = (final_value - initial_value) / initial_value
        
        # è®¡ç®—åŸºå‡†æ”¶ç›Šï¼ˆä¹°å…¥æŒæœ‰ï¼‰
        initial_price = test_data.iloc[0]['Close']
        final_price = test_data.iloc[-1]['Close']
        benchmark_return = (final_price - initial_price) / initial_price
        
        results = {
            'initial_value': initial_value,
            'final_value': final_value,
            'total_return': total_return,
            'benchmark_return': benchmark_return,
            'excess_return': total_return - benchmark_return,
            'actions_taken': actions_taken,
            'portfolio_values': portfolio_values,
            'trades': test_env.trades
        }
        
        print(f"ğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"  åˆå§‹èµ„é‡‘: ${initial_value:,.2f}")
        print(f"  æœ€ç»ˆèµ„é‡‘: ${final_value:,.2f}")
        print(f"  æ€»æ”¶ç›Šç‡: {total_return:.2%}")
        print(f"  åŸºå‡†æ”¶ç›Šç‡: {benchmark_return:.2%}")
        print(f"  è¶…é¢æ”¶ç›Š: {total_return - benchmark_return:.2%}")
        print(f"  äº¤æ˜“æ¬¡æ•°: {len(test_env.trades)}")
        
        return results
    
    def generate_optimized_signals(self, current_data):
        """ç”Ÿæˆä¼˜åŒ–çš„äº¤æ˜“ä¿¡å·"""
        print("ğŸ¯ ç”Ÿæˆå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ä¿¡å·...")
        
        if self.agent is None:
            print("âŒ è¯·å…ˆè®­ç»ƒæ™ºèƒ½ä½“")
            return None
        
        # åˆ›å»ºä¸´æ—¶ç¯å¢ƒè·å–å½“å‰çŠ¶æ€
        temp_env = TradingEnvironment(current_data)
        state = temp_env._get_observation()
        
        # è·å–åŠ¨ä½œå»ºè®®
        self.agent.epsilon = 0  # ä¸æ¢ç´¢ï¼Œä½¿ç”¨æœ€ä½³ç­–ç•¥
        action = self.agent.act(state)
        
        action_map = {0: 'HOLD', 1: 'BUY', 2: 'SELL'}
        signal = action_map[action]
        
        # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºQå€¼å·®å¼‚ï¼‰
        state_key = tuple(np.round(state, 2))
        if state_key in self.agent.q_table:
            q_values = self.agent.q_table[state_key]
            max_q = np.max(q_values)
            second_max_q = np.partition(q_values, -2)[-2]
            confidence = min((max_q - second_max_q) * 100, 95)
        else:
            confidence = 50
        
        result = {
            'signal': signal,
            'confidence': confidence,
            'q_values': q_values.tolist() if state_key in self.agent.q_table else [0, 0, 0],
            'state_features': state.tolist()
        }
        
        print(f"ğŸš¦ å¼ºåŒ–å­¦ä¹ ä¿¡å·: {signal} (ç½®ä¿¡åº¦: {confidence:.1f}%)")
        
        return result

def main():
    """æ¼”ç¤ºå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–åŠŸèƒ½"""
    print("ğŸ¤– å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–æ¼”ç¤º")
    print("Author: LDL")
    print("="*50)
    
    # ç”Ÿæˆæ¼”ç¤ºæ•°æ®
    print("\nğŸ“Š ç”Ÿæˆæ¼”ç¤ºæ•°æ®...")
    dates = pd.date_range(start='2024-01-01', end='2025-01-25', freq='D')
    dates = dates[dates.weekday < 5]
    
    np.random.seed(42)
    prices = [200]
    for i in range(1, len(dates)):
        change = np.random.normal(0.001, 0.02)
        new_price = prices[-1] * (1 + change)
        prices.append(max(new_price, 50))
    
    demo_data = pd.DataFrame({
        'Date': dates,
        'Close': prices,
        'Volume': np.random.randint(20000000, 80000000, len(dates))
    })
    demo_data.set_index('Date', inplace=True)
    
    # åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
    split_point = int(len(demo_data) * 0.8)
    train_data = demo_data.iloc[:split_point]
    test_data = demo_data.iloc[split_point:]
    
    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ")
    print(f"  è®­ç»ƒæ•°æ®: {len(train_data)}å¤©")
    print(f"  æµ‹è¯•æ•°æ®: {len(test_data)}å¤©")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = ReinforcementLearningOptimizer()
    
    # åˆ›å»ºç¯å¢ƒå¹¶è®­ç»ƒ
    optimizer.create_training_environment(train_data)
    scores = optimizer.train_agent(episodes=100)
    
    # æµ‹è¯•ç­–ç•¥
    results = optimizer.test_strategy(test_data)
    
    # ç”Ÿæˆå½“å‰ä¿¡å·
    current_signal = optimizer.generate_optimized_signals(demo_data.tail(50))
    
    print("\nğŸ‰ å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¼”ç¤ºå®Œæˆ!")
    print("\nğŸ’¡ å¼ºåŒ–å­¦ä¹ ç‰¹ç‚¹:")
    print("  âœ… è‡ªåŠ¨ç­–ç•¥ä¼˜åŒ–")
    print("  âœ… ç¯å¢ƒé€‚åº”å­¦ä¹ ")
    print("  âœ… é£é™©æ”¶ç›Šå¹³è¡¡")
    print("  âœ… åŠ¨æ€å‚æ•°è°ƒæ•´")

if __name__ == "__main__":
    main()
