#!/usr/bin/env python3
"""
QuantMuse AIæ™ºèƒ½åŒ–è½»é‡ç‰ˆ
Author: LDL
Date: 2025-01-25

ä½¿ç”¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ å®ç°AIæ™ºèƒ½åŒ–åŠŸèƒ½ï¼Œæ— éœ€TensorFlowä¾èµ–
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

class AIIntelligenceLiteEngine:
    """AIæ™ºèƒ½åŒ–è½»é‡ç‰ˆå¼•æ“"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        self.model_performance = {}
        
        print("ğŸ¤– AIæ™ºèƒ½åŒ–è½»é‡ç‰ˆå¼•æ“åˆå§‹åŒ–å®Œæˆ")
        print("Author: LDL")
        print("="*50)
    
    def create_advanced_features(self, data):
        """åˆ›å»ºé«˜çº§ç‰¹å¾å·¥ç¨‹"""
        df = data.copy()
        
        # åŸºç¡€ä»·æ ¼ç‰¹å¾
        df['returns'] = df['Close'].pct_change()
        df['log_returns'] = np.log(df['Close'] / df['Close'].shift(1))
        df['price_change'] = df['Close'] - df['Close'].shift(1)
        
        # æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
        df['rsi'] = self.calculate_rsi(df['Close'])
        df['macd'], df['macd_signal'] = self.calculate_macd(df['Close'])
        df['bb_upper'], df['bb_middle'], df['bb_lower'] = self.calculate_bollinger_bands(df['Close'])
        
        # ç§»åŠ¨å¹³å‡ç‰¹å¾
        for window in [5, 10, 20, 50]:
            df[f'ma_{window}'] = df['Close'].rolling(window).mean()
            df[f'ma_ratio_{window}'] = df['Close'] / df[f'ma_{window}']
            df[f'ma_slope_{window}'] = df[f'ma_{window}'].diff(5)
        
        # æ³¢åŠ¨ç‡ç‰¹å¾
        for window in [5, 10, 20]:
            df[f'volatility_{window}'] = df['returns'].rolling(window).std()
            df[f'volatility_ratio_{window}'] = df[f'volatility_{window}'] / df[f'volatility_{window}'].rolling(50).mean()
        
        # ä»·æ ¼ä½ç½®ç‰¹å¾
        for window in [20, 50]:
            df[f'price_position_{window}'] = (df['Close'] - df['Close'].rolling(window).min()) / (df['Close'].rolling(window).max() - df['Close'].rolling(window).min())
        
        # æˆäº¤é‡ç‰¹å¾
        df['volume_ma'] = df['Volume'].rolling(20).mean()
        df['volume_ratio'] = df['Volume'] / df['volume_ma']
        df['volume_price_trend'] = df['Volume'] * df['returns']
        
        # åŠ¨é‡ç‰¹å¾
        for period in [1, 3, 5, 10]:
            df[f'momentum_{period}'] = df['Close'] / df['Close'].shift(period) - 1
        
        # é«˜çº§æŠ€æœ¯ç‰¹å¾
        df['williams_r'] = self.calculate_williams_r(df)
        df['stoch_k'], df['stoch_d'] = self.calculate_stochastic(df)
        
        # å¸‚åœºç»“æ„ç‰¹å¾
        df['higher_high'] = (df['High'] > df['High'].shift(1)).astype(int)
        df['lower_low'] = (df['Low'] < df['Low'].shift(1)).astype(int)
        df['inside_bar'] = ((df['High'] < df['High'].shift(1)) & (df['Low'] > df['Low'].shift(1))).astype(int)
        
        return df
    
    def calculate_rsi(self, prices, window=14):
        """è®¡ç®—RSI"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def calculate_macd(self, prices, fast=12, slow=26, signal=9):
        """è®¡ç®—MACD"""
        exp1 = prices.ewm(span=fast).mean()
        exp2 = prices.ewm(span=slow).mean()
        macd = exp1 - exp2
        macd_signal = macd.ewm(span=signal).mean()
        return macd, macd_signal
    
    def calculate_bollinger_bands(self, prices, window=20, num_std=2):
        """è®¡ç®—å¸ƒæ—å¸¦"""
        ma = prices.rolling(window).mean()
        std = prices.rolling(window).std()
        upper = ma + (std * num_std)
        lower = ma - (std * num_std)
        return upper, ma, lower
    
    def calculate_williams_r(self, df, window=14):
        """è®¡ç®—å¨å»‰æŒ‡æ ‡"""
        highest_high = df['High'].rolling(window).max()
        lowest_low = df['Low'].rolling(window).min()
        williams_r = -100 * (highest_high - df['Close']) / (highest_high - lowest_low)
        return williams_r
    
    def calculate_stochastic(self, df, k_window=14, d_window=3):
        """è®¡ç®—éšæœºæŒ‡æ ‡"""
        lowest_low = df['Low'].rolling(k_window).min()
        highest_high = df['High'].rolling(k_window).max()
        k_percent = 100 * (df['Close'] - lowest_low) / (highest_high - lowest_low)
        d_percent = k_percent.rolling(d_window).mean()
        return k_percent, d_percent
    
    def prepare_ml_data(self, df, target_periods=[1, 3, 5]):
        """å‡†å¤‡æœºå™¨å­¦ä¹ æ•°æ®"""
        # åˆ›å»ºå¤šä¸ªé¢„æµ‹ç›®æ ‡
        for period in target_periods:
            df[f'target_{period}d'] = df['Close'].shift(-period) / df['Close'] - 1
        
        # é€‰æ‹©ç‰¹å¾åˆ—
        feature_columns = [col for col in df.columns if col not in ['Close', 'Open', 'High', 'Low', 'Volume'] + [f'target_{p}d' for p in target_periods]]
        
        # ç§»é™¤åŒ…å«NaNçš„è¡Œ
        clean_df = df.dropna()
        
        if len(clean_df) < 50:
            print("âš ï¸ æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆè®­ç»ƒ")
            return None, None, None
        
        X = clean_df[feature_columns]
        y_dict = {f'{p}d': clean_df[f'target_{p}d'] for p in target_periods}
        
        return X, y_dict, feature_columns
    
    def train_ensemble_models(self, X, y, target_name="1d"):
        """è®­ç»ƒé›†æˆæ¨¡å‹"""
        print(f"ğŸ§  è®­ç»ƒ{target_name}é¢„æµ‹çš„é›†æˆæ¨¡å‹...")
        
        # æ•°æ®æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
        
        # å®šä¹‰å¤šä¸ªæ¨¡å‹
        models = {
            'random_forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),
            'gradient_boosting': GradientBoostingRegressor(n_estimators=100, max_depth=6, random_state=42),
            'linear_regression': LinearRegression()
        }
        
        trained_models = {}
        model_scores = {}
        
        for name, model in models.items():
            # è®­ç»ƒæ¨¡å‹
            model.fit(X_train, y_train)
            
            # é¢„æµ‹å’Œè¯„ä¼°
            y_pred = model.predict(X_test)
            r2 = r2_score(y_test, y_pred)
            mse = mean_squared_error(y_test, y_pred)
            
            # äº¤å‰éªŒè¯
            cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')
            
            trained_models[name] = model
            model_scores[name] = {
                'r2_score': r2,
                'mse': mse,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }
            
            print(f"  {name}: RÂ²={r2:.4f}, MSE={mse:.6f}, CV={cv_scores.mean():.4f}Â±{cv_scores.std():.4f}")
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹
        best_model_name = max(model_scores.keys(), key=lambda k: model_scores[k]['cv_mean'])
        best_model = trained_models[best_model_name]
        
        print(f"âœ… æœ€ä½³æ¨¡å‹: {best_model_name}")
        
        return {
            'models': trained_models,
            'best_model': best_model,
            'best_model_name': best_model_name,
            'scaler': scaler,
            'scores': model_scores,
            'feature_names': X.columns.tolist()
        }
    
    def analyze_feature_importance(self, model_info, top_n=10):
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        best_model = model_info['best_model']
        feature_names = model_info['feature_names']
        
        if hasattr(best_model, 'feature_importances_'):
            importances = best_model.feature_importances_
            feature_importance = pd.DataFrame({
                'feature': feature_names,
                'importance': importances
            }).sort_values('importance', ascending=False)
            
            print(f"\nğŸ“Š ç‰¹å¾é‡è¦æ€§åˆ†æ (Top {top_n}):")
            for i, row in feature_importance.head(top_n).iterrows():
                print(f"  {row['feature']}: {row['importance']:.4f}")
            
            return feature_importance
        else:
            print("âš ï¸ å½“å‰æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")
            return None
    
    def generate_ai_predictions(self, data, symbol="TSLA"):
        """ç”ŸæˆAIé¢„æµ‹"""
        print(f"ğŸ”® ä¸º{symbol}ç”ŸæˆAIæ™ºèƒ½é¢„æµ‹...")
        
        # ç‰¹å¾å·¥ç¨‹
        enhanced_data = self.create_advanced_features(data)
        
        # å‡†å¤‡æœºå™¨å­¦ä¹ æ•°æ®
        X, y_dict, feature_columns = self.prepare_ml_data(enhanced_data)
        
        if X is None:
            print("âŒ æ•°æ®å‡†å¤‡å¤±è´¥")
            return None
        
        predictions = {}
        
        # ä¸ºä¸åŒæ—¶é—´å‘¨æœŸè®­ç»ƒæ¨¡å‹
        for period, y in y_dict.items():
            model_info = self.train_ensemble_models(X, y, period)
            
            # åˆ†æç‰¹å¾é‡è¦æ€§
            feature_importance = self.analyze_feature_importance(model_info)
            
            # ç”Ÿæˆæœ€æ–°é¢„æµ‹
            latest_features = X.iloc[-1:].values
            latest_scaled = model_info['scaler'].transform(latest_features)
            
            # é›†æˆé¢„æµ‹
            ensemble_predictions = []
            for name, model in model_info['models'].items():
                pred = model.predict(latest_scaled)[0]
                ensemble_predictions.append(pred)
            
            # åŠ æƒå¹³å‡ï¼ˆåŸºäºæ¨¡å‹æ€§èƒ½ï¼‰
            weights = [model_info['scores'][name]['cv_mean'] for name in model_info['models'].keys()]
            weights = np.array(weights) / sum(weights)
            final_prediction = np.average(ensemble_predictions, weights=weights)
            
            # è®¡ç®—ç½®ä¿¡åº¦
            pred_std = np.std(ensemble_predictions)
            confidence = max(0, min(100, (1 - pred_std * 10) * 100))
            
            predictions[period] = {
                'prediction': final_prediction,
                'confidence': confidence,
                'individual_predictions': dict(zip(model_info['models'].keys(), ensemble_predictions)),
                'best_model': model_info['best_model_name'],
                'model_performance': model_info['scores'],
                'feature_importance': feature_importance.head(5).to_dict('records') if feature_importance is not None else None
            }
            
            print(f"ğŸ“ˆ {period}é¢„æµ‹: {final_prediction:.2%} (ç½®ä¿¡åº¦: {confidence:.1f}%)")
        
        return predictions
    
    def generate_ai_trading_signals(self, predictions, current_price):
        """ç”ŸæˆAIäº¤æ˜“ä¿¡å·"""
        print("ğŸ¯ ç”ŸæˆAIæ™ºèƒ½äº¤æ˜“ä¿¡å·...")
        
        if not predictions:
            return None
        
        # ç»¼åˆå¤šæ—¶é—´å‘¨æœŸçš„é¢„æµ‹
        short_term = predictions.get('1d', {})
        medium_term = predictions.get('3d', {})
        long_term = predictions.get('5d', {})
        
        signals = []
        confidence_scores = []
        
        # çŸ­æœŸä¿¡å·
        if short_term:
            pred = short_term['prediction']
            conf = short_term['confidence']
            if pred > 0.02 and conf > 60:
                signals.append('BUY')
                confidence_scores.append(conf)
            elif pred < -0.02 and conf > 60:
                signals.append('SELL')
                confidence_scores.append(conf)
            else:
                signals.append('HOLD')
                confidence_scores.append(conf * 0.5)
        
        # ä¸­æœŸä¿¡å·
        if medium_term:
            pred = medium_term['prediction']
            conf = medium_term['confidence']
            if pred > 0.05 and conf > 50:
                signals.append('BUY')
                confidence_scores.append(conf * 0.8)
            elif pred < -0.05 and conf > 50:
                signals.append('SELL')
                confidence_scores.append(conf * 0.8)
        
        # ç»¼åˆä¿¡å·
        buy_signals = signals.count('BUY')
        sell_signals = signals.count('SELL')
        
        if buy_signals > sell_signals:
            final_signal = 'BUY'
        elif sell_signals > buy_signals:
            final_signal = 'SELL'
        else:
            final_signal = 'HOLD'
        
        # ç»¼åˆç½®ä¿¡åº¦
        avg_confidence = np.mean(confidence_scores) if confidence_scores else 50
        
        result = {
            'signal': final_signal,
            'confidence': avg_confidence,
            'predictions': predictions,
            'reasoning': {
                'short_term': short_term.get('prediction', 0),
                'medium_term': medium_term.get('prediction', 0),
                'long_term': long_term.get('prediction', 0)
            }
        }
        
        print(f"ğŸš¦ AIä¿¡å·: {final_signal} (ç½®ä¿¡åº¦: {avg_confidence:.1f}%)")
        print(f"ğŸ“Š é¢„æµ‹ç†ç”±:")
        print(f"  çŸ­æœŸ(1å¤©): {result['reasoning']['short_term']:.2%}")
        print(f"  ä¸­æœŸ(3å¤©): {result['reasoning']['medium_term']:.2%}")
        print(f"  é•¿æœŸ(5å¤©): {result['reasoning']['long_term']:.2%}")
        
        return result

def main():
    """æ¼”ç¤ºAIæ™ºèƒ½åŒ–è½»é‡ç‰ˆåŠŸèƒ½"""
    print("ğŸ¤– QuantMuse AIæ™ºèƒ½åŒ–è½»é‡ç‰ˆæ¼”ç¤º")
    print("Author: LDL")
    print("="*50)
    
    # åˆ›å»ºAIå¼•æ“
    ai_engine = AIIntelligenceLiteEngine()
    
    # ç”Ÿæˆæ¼”ç¤ºæ•°æ®
    print("\nğŸ“Š ç”Ÿæˆæ¼”ç¤ºæ•°æ®...")
    dates = pd.date_range(start='2024-01-01', end='2025-01-25', freq='D')
    dates = dates[dates.weekday < 5]
    
    np.random.seed(42)
    prices = [200]
    volumes = []
    
    for i in range(1, len(dates)):
        # æ·»åŠ è¶‹åŠ¿å’Œéšæœºæ€§
        trend = 0.0005  # è½»å¾®ä¸Šå‡è¶‹åŠ¿
        noise = np.random.normal(0, 0.02)
        change = trend + noise
        
        new_price = prices[-1] * (1 + change)
        prices.append(max(new_price, 50))
        volumes.append(np.random.randint(20000000, 80000000))
    
    volumes.insert(0, 50000000)  # ç¬¬ä¸€å¤©çš„æˆäº¤é‡
    
    demo_data = pd.DataFrame({
        'Date': dates,
        'Close': prices,
        'Volume': volumes,
        'High': [p * (1 + np.random.uniform(0, 0.02)) for p in prices],
        'Low': [p * (1 - np.random.uniform(0, 0.02)) for p in prices],
        'Open': [prices[max(0, i-1)] * (1 + np.random.uniform(-0.01, 0.01)) for i in range(len(prices))]
    })
    demo_data.set_index('Date', inplace=True)
    
    print(f"âœ… ç”Ÿæˆäº†{len(demo_data)}æ¡æ¼”ç¤ºæ•°æ®")
    
    # ç”ŸæˆAIé¢„æµ‹
    predictions = ai_engine.generate_ai_predictions(demo_data, "TSLA")
    
    if predictions:
        # ç”Ÿæˆäº¤æ˜“ä¿¡å·
        current_price = demo_data['Close'].iloc[-1]
        trading_signals = ai_engine.generate_ai_trading_signals(predictions, current_price)
        
        print("\nğŸ‰ AIæ™ºèƒ½åŒ–è½»é‡ç‰ˆæ¼”ç¤ºå®Œæˆ!")
        print("\nğŸ’¡ AIåŠŸèƒ½ç‰¹ç‚¹:")
        print("  âœ… é«˜çº§ç‰¹å¾å·¥ç¨‹ (50+ æŠ€æœ¯ç‰¹å¾)")
        print("  âœ… é›†æˆæœºå™¨å­¦ä¹ æ¨¡å‹")
        print("  âœ… å¤šæ—¶é—´å‘¨æœŸé¢„æµ‹")
        print("  âœ… ç‰¹å¾é‡è¦æ€§åˆ†æ")
        print("  âœ… æ™ºèƒ½äº¤æ˜“ä¿¡å·ç”Ÿæˆ")
        print("  âœ… ç½®ä¿¡åº¦è¯„ä¼°")
        
        print("\nğŸš€ ç›¸æ¯”ä¼ ç»Ÿåˆ†æçš„ä¼˜åŠ¿:")
        print("  â€¢ è‡ªåŠ¨ç‰¹å¾å‘ç°")
        print("  â€¢ å¤šæ¨¡å‹é›†æˆé¢„æµ‹")
        print("  â€¢ é‡åŒ–ç½®ä¿¡åº¦")
        print("  â€¢ è‡ªé€‚åº”å­¦ä¹ ")
    
    else:
        print("âŒ AIé¢„æµ‹ç”Ÿæˆå¤±è´¥")

if __name__ == "__main__":
    main()
